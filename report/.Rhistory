actual_response = g_test) +
ggtitle("Multi-class logistic regression")
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
ggtitle("Fully connected neural network")
p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
actual_response = g_test) +
ggtitle("Convolutional neural network")
p1_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_lr,
actual_response = g_test) +
ggtitle("Multi-class logistic regression")
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
ggtitle("Fully connected neural network")
# p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
#                                             actual_response = g_test) +
#   ggtitle("Convolutional neural network")
# use cowplot to concatenate the confusion matrices
plot_grid(p1_conf_matrix, p2_conf_matrix, nrow = 1)
p1_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_lr,
actual_response = g_test) +
labs(main = "Multi-class logistic regression")
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
labs(main = "Fully connected neural network")
# p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
#                                             actual_response = g_test) +
#   ggtitle("Convolutional neural network")
# use cowplot to concatenate the confusion matrices
plot_grid(p1_conf_matrix, p2_conf_matrix, nrow = 1)
p1_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_lr,
actual_response = g_test) +
ggtitle("Multi-class logistic regression")
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
ggtitle("Fully connected neural network")
# p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
#                                             actual_response = g_test) +
#   ggtitle("Convolutional neural network")
# use cowplot to concatenate the confusion matrices
plot_grid(p1_conf_matrix, p2_conf_matrix, nrow = 1)
p1_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_lr,
actual_response = g_test) +
ggtitle("Multi-class logistic regression") +
theme(plot.title = element_text(hjust = 0.5))
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
ggtitle("Fully connected neural network") +
theme(plot.title = element_text(hjust = 0.5))
# p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
#                                             actual_response = g_test) +
#   ggtitle("Convolutional neural network") +
#   theme(plot.title = element_text(hjust = 0.5))
# use cowplot to concatenate the confusion matrices
plot_grid(p1_conf_matrix, p2_conf_matrix, nrow = 1)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(keras)         # to train neural networks
library(kableExtra)    # to print tables
library(cowplot)       # to print side-by-side plots
library(tidyverse)     # tidyverse
source("../../functions/deep_learning_helpers.R")
class_names = tribble(
~class, ~name,
0, "T-shirt/top",
1, "Trouser",
2, "Pullover",
3, "Dress",
4, "Coat",
5, "Sandal",
6, "Shirt",
7, "Sneaker",
8, "Bag",
9, "Ankle boot"
)
class_names %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 2,
col.names = c("Index",
"Name"),
caption = "The ten classes in the Fashion MNIST data.") %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
# load the data
fashion_mnist <- dataset_fashion_mnist()
# extract information about the images
num_classes = nrow(class_names)                   # number of image classes
num_train_images = dim(fashion_mnist$train$x)[1]  # number of training images
num_test_images = dim(fashion_mnist$test$x)[1]    # number of test images
img_rows <- dim(fashion_mnist$train$x)[2]         # rows per image
img_cols <- dim(fashion_mnist$train$x)[3]         # columns per image
num_pixels = img_rows*img_cols                    # pixels per image
max_intensity = 255                               # max pixel intensity
# normalize and reshape the images
x_train <- array_reshape(fashion_mnist$train$x/max_intensity,
c(num_train_images, img_rows, img_cols, 1))
x_test <- array_reshape(fashion_mnist$test$x/max_intensity,
c(num_test_images, img_rows, img_cols, 1))
# extract the responses from the training and test data
g_train <- fashion_mnist$train$y
g_test <- fashion_mnist$test$y
# recode response labels using "one-hot" representation
y_train <- to_categorical(g_train, num_classes)
y_test <- to_categorical(g_test, num_classes)
table(g_train)
# plot the first six training images
p1 = plot_grayscale(x_train[1,,,], g_train[1], class_names)
p2 = plot_grayscale(x_train[2,,,], g_train[2], class_names)
p3 = plot_grayscale(x_train[3,,,], g_train[3], class_names)
p4 = plot_grayscale(x_train[4,,,], g_train[4], class_names)
p5 = plot_grayscale(x_train[5,,,], g_train[5], class_names)
p6 = plot_grayscale(x_train[6,,,], g_train[6], class_names)
# use cowplot to concatenate the six training images
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
# define model object
model_lr <- keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = num_classes,
activation = "softmax")
# compile model
model_lr %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy"))
summary(model_lr)
# train the model for 10 epochs
model_lr_history = model_lr %>% fit(x_train,
y_train,
epochs = 10,
batch_size = 128,
validation_split = 0.2)
# save model
save_model_hdf5(model_lr, "model_lr.h5")
# save history
saveRDS(model_lr$history$history, "model_lr_hist.RDS")
# load model
model_lr = load_model_hdf5("model_lr.h5")
# load history
model_lr_hist = readRDS("model_lr_hist.RDS")
# plot the history
plot_model_history(model_lr_hist)
# define model for fully connected neural network with three hidden layers
model_nn = keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax")
# compile model
model_nn %>% compile(loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(model_nn)
plot_model_history(model_nn_hist)
plot_model_history(model_nn_history)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn$history$history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(keras)         # to train neural networks
library(kableExtra)    # to print tables
library(cowplot)       # to print side-by-side plots
library(tidyverse)     # tidyverse
source("../../functions/deep_learning_helpers.R")
class_names = tribble(
~class, ~name,
0, "T-shirt/top",
1, "Trouser",
2, "Pullover",
3, "Dress",
4, "Coat",
5, "Sandal",
6, "Shirt",
7, "Sneaker",
8, "Bag",
9, "Ankle boot"
)
class_names %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 2,
col.names = c("Index",
"Name"),
caption = "The ten classes in the Fashion MNIST data.") %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
# load the data
fashion_mnist <- dataset_fashion_mnist()
# extract information about the images
num_classes = nrow(class_names)                   # number of image classes
num_train_images = dim(fashion_mnist$train$x)[1]  # number of training images
num_test_images = dim(fashion_mnist$test$x)[1]    # number of test images
img_rows <- dim(fashion_mnist$train$x)[2]         # rows per image
img_cols <- dim(fashion_mnist$train$x)[3]         # columns per image
num_pixels = img_rows*img_cols                    # pixels per image
max_intensity = 255                               # max pixel intensity
# normalize and reshape the images
x_train <- array_reshape(fashion_mnist$train$x/max_intensity,
c(num_train_images, img_rows, img_cols, 1))
x_test <- array_reshape(fashion_mnist$test$x/max_intensity,
c(num_test_images, img_rows, img_cols, 1))
# extract the responses from the training and test data
g_train <- fashion_mnist$train$y
g_test <- fashion_mnist$test$y
# recode response labels using "one-hot" representation
y_train <- to_categorical(g_train, num_classes)
y_test <- to_categorical(g_test, num_classes)
table(g_train)
# plot the first six training images
p1 = plot_grayscale(x_train[1,,,], g_train[1], class_names)
p2 = plot_grayscale(x_train[2,,,], g_train[2], class_names)
p3 = plot_grayscale(x_train[3,,,], g_train[3], class_names)
p4 = plot_grayscale(x_train[4,,,], g_train[4], class_names)
p5 = plot_grayscale(x_train[5,,,], g_train[5], class_names)
p6 = plot_grayscale(x_train[6,,,], g_train[6], class_names)
# use cowplot to concatenate the six training images
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
# define model object
model_lr <- keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = num_classes,
activation = "softmax")
# compile model
model_lr %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy"))
summary(model_lr)
# train the model for 10 epochs
model_lr_history = model_lr %>% fit(x_train,
y_train,
epochs = 10,
batch_size = 128,
validation_split = 0.2)
# save model
save_model_hdf5(model_lr, "model_lr.h5")
# save history
saveRDS(model_lr$history$history, "model_lr_hist.RDS")
# load model
model_lr = load_model_hdf5("model_lr.h5")
# load history
model_lr_hist = readRDS("model_lr_hist.RDS")
# plot the history
plot_model_history(model_lr_hist)
# define model for fully connected neural network with three hidden layers
model_nn = keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax")
# compile model
model_nn %>% compile(loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy"))
summary(model_nn)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn$history$history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn_history, "model_nn_hist.RDS")
# train the model using 15 epochs
model_nn_history = model_nn %>%
fit(x_train,
y_train,
epochs = 15,
batch_size = 128,
validation_split = 0.2)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn_history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# save history
saveRDS(model_nn$history$history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
View(model_nn_history)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn_history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
plot_model_history(model_nn$history$history)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn_history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(keras)         # to train neural networks
library(kableExtra)    # to print tables
library(cowplot)       # to print side-by-side plots
library(tidyverse)     # tidyverse
source("../../functions/deep_learning_helpers.R")
class_names = tribble(
~class, ~name,
0, "T-shirt/top",
1, "Trouser",
2, "Pullover",
3, "Dress",
4, "Coat",
5, "Sandal",
6, "Shirt",
7, "Sneaker",
8, "Bag",
9, "Ankle boot"
)
class_names %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 2,
col.names = c("Index",
"Name"),
caption = "The ten classes in the Fashion MNIST data.") %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
# load the data
fashion_mnist <- dataset_fashion_mnist()
# extract information about the images
num_classes = nrow(class_names)                   # number of image classes
num_train_images = dim(fashion_mnist$train$x)[1]  # number of training images
num_test_images = dim(fashion_mnist$test$x)[1]    # number of test images
img_rows <- dim(fashion_mnist$train$x)[2]         # rows per image
img_cols <- dim(fashion_mnist$train$x)[3]         # columns per image
num_pixels = img_rows*img_cols                    # pixels per image
max_intensity = 255                               # max pixel intensity
# normalize and reshape the images
x_train <- array_reshape(fashion_mnist$train$x/max_intensity,
c(num_train_images, img_rows, img_cols, 1))
x_test <- array_reshape(fashion_mnist$test$x/max_intensity,
c(num_test_images, img_rows, img_cols, 1))
# extract the responses from the training and test data
g_train <- fashion_mnist$train$y
g_test <- fashion_mnist$test$y
# recode response labels using "one-hot" representation
y_train <- to_categorical(g_train, num_classes)
y_test <- to_categorical(g_test, num_classes)
table(g_train)
# plot the first six training images
p1 = plot_grayscale(x_train[1,,,], g_train[1], class_names)
p2 = plot_grayscale(x_train[2,,,], g_train[2], class_names)
p3 = plot_grayscale(x_train[3,,,], g_train[3], class_names)
p4 = plot_grayscale(x_train[4,,,], g_train[4], class_names)
p5 = plot_grayscale(x_train[5,,,], g_train[5], class_names)
p6 = plot_grayscale(x_train[6,,,], g_train[6], class_names)
# use cowplot to concatenate the six training images
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
# define model object
model_lr <- keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = num_classes,
activation = "softmax")
# compile model
model_lr %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy"))
summary(model_lr)
# train the model for 10 epochs
model_lr_history = model_lr %>% fit(x_train,
y_train,
epochs = 10,
batch_size = 128,
validation_split = 0.2)
# save model
save_model_hdf5(model_lr, "model_lr.h5")
# save history
saveRDS(model_lr$history$history, "model_lr_hist.RDS")
# load model
model_lr = load_model_hdf5("model_lr.h5")
# load history
model_lr_hist = readRDS("model_lr_hist.RDS")
# plot the history
plot_model_history(model_lr_hist)
# define model for fully connected neural network with three hidden layers
model_nn = keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax")
# compile model
model_nn %>% compile(loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy"))
summary(model_nn)
# train the model using 15 epochs
model_nn_history = model_nn %>%
fit(x_train,
y_train,
epochs = 15,
batch_size = 128,
validation_split = 0.2)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn_history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn$history$history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
# get predicted classes for lr
pred_classes_lr = model_lr %>% predict(x_test) %>% k_argmax() %>%
as.integer()
accuracy_lr = mean(pred_classes_lr == g_test)
# get predicted classes for nn
pred_classes_nn = model_nn %>% predict(x_test) %>% k_argmax() %>%
as.integer()
accuracy_nn = mean(pred_classes_nn == g_test)
accuracy_cnn = 0;
# create table of model metrics
error_for_models = tribble(
~model, ~accuracy, ~num_layers, ~num_params, ~ms_per_sgds,
#------/---------/------------/-------------/--------------
"Multi-class logistic regression", accuracy_lr, 1, 7850, 5,
"Fully connected neural network", accuracy_nn, 4, 242762, 10,
"Convolutional neural network", accuracy_cnn, 0, 0, 0,
"Human performance", 0.835, NA, NA, NA,
)
# print these metrics in table
error_for_models %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 5,
col.names = c("Model type",
"Test accuracy",
"Number of layers",
"Number of parameters",
"Milliseconds per stochastic
gradient descent step"),
caption = "These are the model metrics for the
three trained neural network models as well as a
comparison against human performance.") %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
error_for_models
p1_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_lr,
actual_response = g_test) +
ggtitle("Multi-class logistic regression") +
theme(plot.title = element_text(hjust = 0.5))
p2_conf_matrix = plot_confusion_matrix(predicted_responses = pred_classes_nn,
actual_response = g_test) +
ggtitle("Fully connected neural network") +
theme(plot.title = element_text(hjust = 0.5))
# p3_confusion_matrix = plot_confusion_matrix(predicted_responses = pred_classes_cnn,
#                                             actual_response = g_test) +
#   ggtitle("Convolutional neural network") +
#   theme(plot.title = element_text(hjust = 0.5))
# use cowplot to concatenate the confusion matrices
plot_grid(p1_conf_matrix, p2_conf_matrix, nrow = 1)
setwd("~/Desktop/Fall 2021/STAT 471/final-project/report")
setwd("~/")
setwd("~/Desktop/Fall 2021/STAT 471/final-project/report")
